
# üß† ai-agent

Agente IA razonador construido con **LangChain** + **OpenAI** (GPT-4).  
Orquesta tareas complejas de DevOps mediante razonamiento LLM y llamada modular a herramientas IA propias del ecosistema `devops-ai-lab`.

---

## üéØ Prop√≥sito

Centraliza la l√≥gica de decisi√≥n y automatizaci√≥n DevOps basada en LLMs, encapsulando como **tools** los microservicios IA desplegados v√≠a `ai-gateway`.  
Permite analizar logs, validar charts Helm, generar Jenkinsfiles y recuperar contexto desde una base vectorial documentacional.  
Este agente es la **entrada cognitiva real de la arquitectura**.

---

## üîß Funcionalidad

El agente expone varias **tools** conectadas a servicios internos o externos:

| Tool                    | Funci√≥n                                                                 |
|-------------------------|-------------------------------------------------------------------------|
| `generate_pipeline`     | Genera Jenkinsfile desde texto natural v√≠a `ai-pipeline-gen`            |
| `analyze_log`           | Diagn√≥stico inteligente de logs CI/CD con `ai-log-analyzer`             |
| `lint_chart`            | Linting sem√°ntico de Helm Charts comprimidos (.tgz)                     |
| `analyze_helm_chart`    | Clona, empaqueta y analiza un Helm Chart desde GitHub autom√°ticamente   |
| `list_github_repos`     | Lista todos los repos p√∫blicos de `dorado-ai-devops`                    |
| `query_vector_db`       | Recupera contexto sem√°ntico desde `ai-vector-db` usando b√∫squeda LLM    |

Todas las tools inyectan `caller: ai-agent-langchain` para trazabilidad MCP.

---

## ‚öôÔ∏è Estructura del Proyecto

```
ai-agent/
‚îú‚îÄ‚îÄ main.py                         # Punto de entrada del agente (FastAPI)
‚îú‚îÄ‚îÄ tools/                          # Tools expuestas al agente
‚îÇ   ‚îú‚îÄ‚îÄ ai_gateway_tools.py         # Wrappers HTTP a los microservicios
‚îÇ   ‚îú‚îÄ‚îÄ github_tools.py             # Listado de repositorios desde GitHub
‚îÇ   ‚îú‚îÄ‚îÄ helm_chart_tools.py         # Fetch + lint autom√°tico de Helm Charts
‚îÇ   ‚îî‚îÄ‚îÄ vector_db_tools.py          # Consulta sem√°ntica al vector DB
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îî‚îÄ‚îÄ examples.md                 # Ejemplos y frases de prueba
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ settings.py                 # Configuraci√≥n general
‚îú‚îÄ‚îÄ chart_example/
‚îÇ   ‚îî‚îÄ‚îÄ helm-log-analyzer-0.1.5.tgz
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

---

## üåê API REST

El agente corre en un servidor FastAPI con el endpoint:

- `POST /ask`  
  Payload: `{"prompt": "tu pregunta en lenguaje natural"}`  
  Devuelve: `{"result": "respuesta generada (tool o razonamiento)"}`

El agente identifica si necesita contexto, ejecuta tools, y resume resultados.

---

## üìö Contexto Sem√°ntico (Vector DB)

`query_vector_db` permite que el agente acceda a contexto t√©cnico/documental recuperado sem√°nticamente desde la base vectorial `ai-vector-db`, la cual indexa documentaci√≥n de todos los microservicios del ecosistema `devops-ai-lab`.

El agente puede consultar conceptos como:

```
¬øQu√© hace ai-chat-ui?
¬øD√≥nde est√° implementado el fetch de charts?
```

Y usar√° la respuesta como contexto antes de contestar.

---

## üß† Inteligencia y Modelos

- **Modelo por defecto:** OpenAI GPT-4 (via `langchain_openai.ChatOpenAI`)
- **Motor IA de los tools:** configurable (`ollama`, `openai`) seg√∫n `AI_VENDOR`

---

## üöÄ Ejecuci√≥n Local

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python main.py
```

Aseg√∫rate de tener acceso al `ai-gateway`, y de definir las variables de entorno necesarias (`GH_SECRET`, `AI_VENDOR`, etc.).

---

## üîé Observabilidad y Trazabilidad

- **Logs** en consola para cada tool: inicio, input, resultado, error.
- **MCP (Message Control Plane)**: mensajes emitidos desde `ai-gateway` con trazabilidad por caller, microservicio y petici√≥n.

---

## üì¶ Dependencias principales

```text
langchain
langchain-openai
aiohttp
fastapi
openai
requests
uvicorn
pydantic
python-dotenv
```

> A√±ade `ollama-client` si invocas Ollama localmente.

---

## üìå Estado actual

‚úÖ Funcional  
üß™ En constante expansi√≥n (vector-db, fine-tuning, validaci√≥n avanzada)

---

## üë®‚Äçüíª Autor

**Dani**  
[github.com/dorado-ai-devops](https://github.com/dorado-ai-devops)

---

## üõ° Licencia

Licencia P√∫blica General GNU v3.0


# Descripci√≥n de la infraestructura `DEVOPS-Agent-LAB`

`DEVOPS-Agent-LAB` es una infraestructura modular orientada a automatizar tareas DevOps mediante agentes de lenguaje (`LangChain`) integrados con herramientas personalizadas y capacidades de razonamiento sobre c√≥digo, logs, y despliegues. El sistema opera en un entorno GitOps (ArgoCD + Kubernetes) y ofrece una experiencia unificada para usuarios humanos, pipelines de CI/CD y agentes LLM.

## Componentes principales

### 1. Entrada natural (AI-CHAT-UI)
Interfaz de lenguaje natural que permite a usuarios humanos interactuar con el agente mediante texto libre.

### 2. Raz√≥n y orquestaci√≥n (ai-agent / LangChain)
N√∫cleo cognitivo del sistema. Utiliza LangChain para decidir qu√© herramientas ejecutar, en qu√© orden, y con qu√© contexto. Este agente dispone de herramientas registradas como:

- `generate_pipeline`
- `analyze_log`
- `lint_chart`
- `query_vector_db`
- `analyze_helm_chart`
- `list_github_repos`

### 3. Base de contexto (ai-vector-db / Chroma)
Vector DB que almacena documentos t√©cnicos como c√≥digo, manifiestos de despliegue o logs anteriores. El agente consulta esta base para obtener contexto relevante y enriquecer su razonamiento.

### 4. Herramientas externas personalizadas (EXTERNAL-CUSTOM-TOOLS)
Conjunto de microservicios desplegados en Flask:

- `ai-gateway`: intermediario HTTP para adaptadores externos
- `ai-logs-analyze`: analiza logs CI/CD
- `ai-helm-linter`: valida charts Helm
- `ai-pipeline-gen`: genera Jenkinsfiles desde descripciones

Estas tools pueden trabajar con modelos locales v√≠a `ollama` (ej. Mistral 7B) o con APIs externas como OpenAI si se activa el fallback.

### 5. Integraci√≥n con Jenkins
Jenkins act√∫a como emisor de tareas para el agente (`/agentquery`) y tambi√©n como consumidor de los resultados generados por herramientas del sistema.

### 6. Observabilidad y feedback (FastAPI MCP + Streamlit)
- `ai-mcp-server`: almacena y publica mensajes y eventos en formato estructurado
- `streamlit`: interfaz de visualizaci√≥n para monitorear respuestas, outputs del LLM y estado general del sistema
- Persistencia: los outputs se almacenan en vol√∫menes `/mnt/data/gateway` y `/mnt/data/mcp` seg√∫n origen

### 7. Despliegue GitOps (Kubernetes + ArgoCD)
Todos los componentes son desplegados mediante manifiestos Git y gestionados con ArgoCD sobre Kubernetes.

