# ğŸ§  ai-agent

Agente IA razonador construido con LangChain. Orquesta tareas complejas de DevOps mediante razonamiento simbÃ³lico e invocaciÃ³n modular de herramientas IA ya existentes en el ecosistema `devops-ai-lab`.

---

## ğŸ¯ PropÃ³sito

Centraliza la lÃ³gica de razonamiento y decisiÃ³n basada en LLMs, encapsulando como herramientas (`tools`) los microservicios IA accesibles vÃ­a `ai-gateway`. Permite analizar logs, validar charts, generar pipelines y realizar consultas libres desde un Ãºnico punto de entrada cognitivo.

Este componente marca el inicio de la capa de inteligencia real en la arquitectura.

---

## ğŸ”§ Funcionalidad

El agente combina mÃºltiples herramientas como funciones invocables desde lenguaje natural. Cada tool actÃºa como wrapper HTTP a uno de los siguientes endpoints:

| Tool              | Microservicio        | DescripciÃ³n |
|------------------|----------------------|-------------|
| `LogAnalyzerTool` | `ai-logs-analyze`     | DiagnÃ³stico de logs CI/CD con LLM |
| `HelmLinterTool`  | `ai-helm-linter`      | ValidaciÃ³n semÃ¡ntica de charts Helm |
| `PipelineGenTool` | `ai-pipeline-gen`     | GeneraciÃ³n de Jenkinsfile desde texto |


---

## âš™ï¸ Estructura del Proyecto

```
ai-agent/
â”œâ”€â”€ main.py                       # Entry point: ejecuta el agente LangChain
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ai_gateway_tools.py
â”œâ”€â”€ clients/
â”‚   â””â”€â”€ gateway_client.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ examples.md
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ Dockerfile
```

---

## ğŸš€ EjecuciÃ³n Local

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python main.py
```

Interactuar vÃ­a CLI directamente con el agente:

```
> Â¿QuÃ© necesitas? > Diagnostica este log: (...)
> Resultado: Parece un error de volumen persistente en tu pod de Kubernetes...
```

---

## ğŸŒ ComunicaciÃ³n con el entorno

El agente se conecta a `ai-gateway` para acceder a las herramientas IA existentes. Los endpoints utilizados son:

- `POST /analyze-log`
- `POST /lint-chart`
- `POST /generate-pipeline`
- `POST /query-ollama`
- `POST /query-flan`

(Ver configuraciÃ³n en `config/settings.py`)

---

## ğŸ§  Inteligencia y Modelos

- Modelo principal: `Mistral` vÃ­a Ollama
- Soporte para OpenAI (fallback opcional)
- Soporte futuro: LLM fine-tuneado (FlanT5)

---

## ğŸ“¦ Dependencias

```text
langchain
requests
python-dotenv
ollama-client  # si se usa cliente especÃ­fico
```

---

## ğŸ“Œ Estado actual

- [x] Estructura del agente definida
- [ ] Tools implementadas (en progreso)
- [ ] CLI operativa
- [ ] IntegraciÃ³n completa con `ai-gateway`
- [ ] OrquestaciÃ³n simbÃ³lica multi-tool

---

## ğŸ‘¨â€ğŸ’» Autor

**Dani** â€“ [@dorado-ai-devops](https://github.com/dorado-ai-devops)

---

## ğŸ›¡ Licencia

Licencia PÃºblica General GNU v3.0
