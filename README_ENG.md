# ğŸ§  ai-agent

AI reasoning agent built with **LangChain** + **OpenAI** (GPT-4).\
Orchestrates complex DevOps tasks using LLM-based reasoning and modular tool invocation from the `devops-ai-lab` ecosystem.

---

## ğŸ¯ Purpose

Centralizes LLM-driven DevOps decision logic and automation by encapsulating internal AI microservices as **tools** via `ai-gateway`.\
Enables unified log analysis, Helm chart validation, and Jenkinsfile generation with advanced traceability and observability (MCP).

This agent serves as the true cognitive entry point of the architecture.

---

## ğŸ”§ Functionality

The agent exposes several **tools** acting as HTTP wrappers to AI microservices deployed on Kubernetes:

| Tool                | Gateway Endpoint      | Description                                                         |
| ------------------- | --------------------- | ------------------------------------------------------------------- |
| `generate_pipeline` | `/generate-pipeline`  | Generates Jenkinsfile from a natural language description           |
| `analyze_log`       | `/analyze-log`        | Diagnoses and solves Jenkins/CI logs using LLM                      |
| `lint_chart`        | `/lint-chart`         | Semantic linting of compressed Helm Charts (.tgz)                   |

All tools inject the field `caller: ai-agent-langchain` into the payload for traceability.

---

## âš™ï¸ Project Structure

```
ai-agent/
â”œâ”€â”€ main.py                         # Entry point: runs the LangChain agent and orchestrates tools
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ai_gateway_tools.py         # Definition and HTTP logic for each tool
â”œâ”€â”€ clients/
â”‚   â””â”€â”€ gateway_client.py           # (optional, for extended HTTP client logic)
â”œâ”€â”€ chart_example/
â”‚   â””â”€â”€ helm-log-analyzer-0.1.5.tgz # Example chart for linting tests
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ examples.md
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ Dockerfile
```

---

## ğŸš€ Local Execution

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python main.py
```

You can interact with the agent via CLI or modify the main file to run direct tests on the tools.

---

## ğŸŒ Environment Communication

The agent communicates with `ai-gateway` through internal HTTP requests (Kubernetes service).\
Active endpoints:

- `POST /generate-pipeline`
- `POST /analyze-log`
- `POST /lint-chart`

All prompts, responses, and events are logged by the gateway and audited via MCP messages.

---

## ğŸ§  Intelligence & Models

- **Default model:** OpenAI GPT-4 (can be switched to Mistral/Ollama by changing the config and backend microservice).
- All tool calls use `ollama` as the default engine (configurable).
- Future: integration of a fine-tuned model (`flan-t5` or others).

---

## ğŸ” Observability & Traceability

- **Detailed logs** at each step (start, input, result, error) in stdout (visible in Kubernetes pods).
- **MCP**: All relevant executions are logged by `ai-gateway` through MCP messages, including metadata (caller, paths, microservice, etc).

---

## ğŸ“¦ Main Dependencies

```text
langchain
openai
aiohttp
pydantic
python-dotenv
```

> Add `ollama-client` only if you need to invoke Ollama models locally.

---

## ğŸ“Œ Current Status

-

---

## ğŸ‘¨â€ğŸ’» Author

**Dani**\
[github.com/dorado-ai-devops](https://github.com/dorado-ai-devops)

---

## ğŸ›¡ License

GNU General Public License v3.0
